{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of word : 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "tokenizer = Okt()  \n",
    "bow = \"최고의 음식점에서 중국 및 한국 그리고 이탈리아의 음식인 짜장면 불고기 파스타 등등을 먹었다\"\n",
    "text1 = \"최고의 이탈리안 음식점에서 최고의 파스타를 먹었다\"\n",
    "text2 = \"중국 음식점에서 최고의 짜장면을 먹었다\"\n",
    "text3 = \"한국 음식점에서 최고의 불고기를 먹었다\"\n",
    "text4 = \"최고 최고 중국집\"\n",
    "text5 = \"중국 인공지능 언어 최고\"       # 노이즈!!! 나는 음식에 대해서만 찾고 있는데~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow = ['한국','중국','이탈리아','짜장면','짬뽕','']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['최고', '의', '음식점', '에서', '중국', '및', '한국', '그리고', '이탈리아', '의', '음식', '인', '짜장면', '불고기', '파스타', '등등', '을', '먹었다']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.morphs(bow)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'최고': 0, '의': 1, '음식점': 2, '에서': 3, '중국': 4, '및': 5, '한국': 6, '그리고': 7, '이탈리아': 8, '음식': 9, '인': 10, '짜장면': 11, '불고기': 12, '파스타': 13, '등등': 14, '을': 15, '먹었다': 16}\n"
     ]
    }
   ],
   "source": [
    "# 단어-인덱스 딕셔너리\n",
    "word_to_index = {}\n",
    "\n",
    "# 토큰을 인덱스로 변환\n",
    "for token in tokens:\n",
    "    if token not in word_to_index.keys():\n",
    "        word_to_index[token] = len(word_to_index)\n",
    "        \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0]*len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'최고': 0, '의': 1, '음식점': 2, '에서': 3, '중국': 4, '및': 5, '한국': 6, '그리고': 7, '이탈리아': 8, '음식': 9, '인': 10, '짜장면': 11, '불고기': 12, '파스타': 13, '등등': 14, '을': 15, '먹었다': 16}\n",
      "최고의 이탈리안 음식점에서 최고의 파스타를 먹었다: [2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]\n",
      "중국 음식점에서 최고의 짜장면을 먹었다: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
      "한국 음식점에서 최고의 불고기를 먹었다: [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "최고 최고 중국집: [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "중국 인공지능 언어 최고: [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# BoW로 변환\n",
    "def convert_bow(sentence, word_to_index):\n",
    "    \n",
    "    # 벡터를 단어의 개수만큼 0으로 초기화\n",
    "    vector = [0]*(len(word_to_index))\n",
    "\n",
    "    # 문장을 토큰으로 분리\n",
    "    tokenizer = Okt()\n",
    "    tokens = tokenizer.morphs(sentence)\n",
    "    \n",
    "    # 단어의 인덱스 위치에 1 설정\n",
    "    for token in tokens:\n",
    "        if token in word_to_index.keys():\n",
    "            vector[word_to_index[token]] += 1\n",
    "    \n",
    "    return vector\n",
    "\n",
    "print(word_to_index)\n",
    "print(text1, end=': ')\n",
    "print(convert_bow(text1, word_to_index))\n",
    "print(text2, end=': ')\n",
    "print(convert_bow(text2, word_to_index))\n",
    "print(text3, end=': ')\n",
    "print(convert_bow(text3, word_to_index))\n",
    "print(text4, end=': ')\n",
    "print(convert_bow(text4, word_to_index))\n",
    "print(text5, end=': ')\n",
    "print(convert_bow(text5, word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수 사용: CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'최고 의 음식점 에서 중국 및 한국 그리고 이탈리아 의 음식 인 짜장면 불고기 파스타 등등 을 먹었다'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰을 문자열로 변환\n",
    "sentence = \" \".join(tokens)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['최고 의 음식점 에서 중국 및 한국 그리고 이탈리아 의 음식 인 짜장면 불고기 파스타 등등 을 먹었다']\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer의 입력에 맞게 배열로 변경\n",
    "sentences = []\n",
    "sentences.append(sentence)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'최고': 14, '의': 9, '음식점': 8, '에서': 5, '중국': 12, '및': 3, '한국': 16, '그리고': 0, '이탈리아': 10, '음식': 7, '인': 11, '짜장면': 13, '불고기': 4, '파스타': 15, '등등': 1, '을': 6, '먹었다': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  # Bow 를 만들기 위해서, CouontVectorizer를 쓴다.\n",
    "\n",
    "# 1글자도 인식이 되도록 토큰 패턴 변경\n",
    "cv = CountVectorizer(token_pattern = r\"(?u)\\b\\w+\\b\") # \\d 숫자 \\w 워드\n",
    "cv.fit(sentences)\n",
    "\n",
    "print(cv.vocabulary_) # vocabulary_ : 순서가 바뀌기는 했지만, 토큰이 들어오면 그거를 분리해서 인덱싱을 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer로 변환\n",
    "def convert_cv(sentence, cv):\n",
    "    \n",
    "    # 문장을 토큰으로 분리\n",
    "    tokenizer = Okt()\n",
    "    tokens = tokenizer.morphs(sentence)\n",
    "    \n",
    "    # 토큰을 문자열로 변환\n",
    "    sentence = \" \".join(tokens)\n",
    "    \n",
    "    # CountVectorizer의 입력에 맞게 배열로 변경\n",
    "    sentences = []\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "    # 벡터 변환\n",
    "    vector = cv.transform(sentences).toarray()    \n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최고의 이탈리안 음식점에서 최고의 파스타를 먹었다\n",
      "중국 음식점에서 최고의 짜장면을 먹었다\n",
      "한국 음식점에서 최고의 불고기를 먹었다\n",
      "최고 최고 중국집\n"
     ]
    }
   ],
   "source": [
    "print(text1)\n",
    "print(text2)\n",
    "print(text3)\n",
    "print(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1 0 0 1 2 0 0 0 0 2 1 0]]\n",
      "[[0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0]]\n",
      "[[0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(convert_cv(text1, cv)) # Bow에 매칭이 되겠금, array로 나온다.\n",
    "print(convert_cv(text2, cv))\n",
    "print(convert_cv(text3, cv))\n",
    "print(convert_cv(text4, cv))\n",
    "print(convert_cv(text5, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cos 유사도 직접 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7637626158259734\n"
     ]
    }
   ],
   "source": [
    "# 코사인 시밀리러티를 통해서, 문장간이 얼마나 유사한지 확인할 수 있다.\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "print(cos_sim(convert_cv(text1, cv)[0], convert_cv(text3, cv)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cos 유사도 함수 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76376262]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_sim = cosine_similarity(convert_cv(text1, cv), convert_cv(text3, cv))\n",
    "cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습] Bow 기반 유사도 분석해보기\n",
    "(선택)\n",
    "1) 어제 크롤링한 문서 사용\n",
    "2) 오늘 실습 text 변환해서 사용\n",
    "\n",
    "(순서)\n",
    " * 단어 꾸러미 만들기\n",
    " * indexting 하기\n",
    " * BoW기반 카운트 계산하기\n",
    " *  Cos유사도로 문장/문서간 유사도 비교해보기\n",
    " *  문제점 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
